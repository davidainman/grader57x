\documentclass[12pt]{article}
\renewcommand{\labelenumi}{\arabic{enumi}.} 
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}}
\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}}
\usepackage{enumitem, comment}
\usepackage[hmargin={1in}, vmargin={1in}]{geometry}
\setlist{  
  listparindent=\parindent,
  parsep=0pt,
}
\begin{document}

\title{Documentation for 57x Grading}
\author{David Inman}
\date{\today}
\maketitle

\tableofcontents{}

\section{Introduction}
This document is meant to guide professors and TAs when creating and grading homework assignments for the 57x series. It is roughly structured according to the homework workflow: creating the homework description (\S\ref{sec:homework}), creating the rubric (\S\ref{sec:rubric}), creating the necessary files for grading (\S\ref{sec:files}), setting up the homework checker for student use (\S\ref{sec:checker}), and the use of the automated grader (\S\ref{sec:autograder}). Finally, I have included some documentation for the structure of the grader and checker (\S\ref{sec:documentation}) and future development (\S\ref{sec:future}).

\section{Homework Descriptions} \label{sec:homework}
There are a series of common mistakes students make when doing the homework for the 57X series. This is a set of best practices when writing the homework descriptions to minimize student confusion and errors, and make the homework easier to grade. The development of homework descriptions is presumed to be done by the professor.

\begin{enumerate}
\item {Describe separate components of the homework separately. 

The best practice for clarity is to separate different parts of the assignment by section. If possible, describe the expected function of the program (Viterbi, classification, etc) in one section, and then describe the format of the output in a separate section. Try to minimize component interleaving in the homework description.}

\item {Homeworks should include an at-a-glance summary of expected turned-in files.}

\item {Points should be given either in section headers or in a summary at the end. The points should add up to 75, the portion of the rubric assigned for homework-specific grading (\S\ref{sec:specificrubric}).}

\item {Points should be awarded for program output, not for code structure.
  
  This prevents the TA from needing to review the code for each student. For instance, ``Program outputs a valid path (10 points)" is preferable to ``Initializes lattice (10 points)."}
 
\item {Graded outputs should have a strict format.
  
  This is particularly true within a line. The automated grader can sort lines alphabetically, but it cannot sort formats within a line. For instance, ``On each line, print the probabilities for each class, \texttt{c1 prob1, c2 prob2, ...}" is too vague. ``On each line, print the probabilities for each class in alphabetical order, \texttt{c1 prob1, c2 prob2, ...}" is better.
  
  For every assignment, output should be uniform for all student code on a given input.}

\end{enumerate}

\section{Rubric} \label{sec:rubric}

The rubric across 570, 571, and 572 has been standardized. There are two parts: the standard component, which is graded automatically and identically across all assignments, and then the homework-specific portion, which is defined for each assignment.

\subsection{Standard Portion (25 points)} \label{sec:standardrubric}

There is no deviation for this component, and it is graded automatically by the grader (\S\ref{sec:autograder}). It is graded as follows:

\begin{enumerate}
\item Submitted Files (10 points)
  \begin{enumerate}
    \item Tarfile submitted (2 points)
    \begin{enumerate}
      \item Exists (1 point)
      \item Correctly named (1 point)
    \end{enumerate}
    \item Readme file submitted (2 points)
    \begin{enumerate}
      \item Exists (1 point)
      \item Correctly named (1 point)
    \end{enumerate}
    \item Requested files exist in tarfile (6 points)
  \end{enumerate}
  \item Program runs as expected (15 points)
  \begin{enumerate}
    \item Code runs to completion on input (10 points)
    \item Output of running code matches turned-in files (5 points)
  \end{enumerate}
  \item Extra points off
  \begin{enumerate}
    \item Carriage returns in turned-in output (-2 points)
  \end{enumerate}
\end{enumerate}

\subsection{Homework-specific Portion (75 points)} \label{sec:specificrubric}

This section is based on the needs of each assignment. Although the gross distribution of points is defined by the professor in the homework description, the TA still has to make decisions about the finer distribution.

\begin{enumerate}
\item {Some points should be assigned for correct formatting for each output file.}
 
\item {Points distributed for the program being implemented correctly should ideally depend on evaluating the program output on test files that are as modular as possible.

For instance, the grader could create an input that will test the code's ability to handle multiple labeling paths, and assign points for ``Chooses the correct labeling path" based on that test. Then a separate test may handle unknown words. These test cases should not be intermixed (or there should be a separate test input that mixes them). }
 
\end{enumerate}


\section{Creating Files for Grading} \label{sec:files}

\subsection{List of files needed} \label{sec:fileslist}

For each assignment, a series of files needs to be generated for the purposes of grading. This is an attempt to list all the necessary files. The first file (submit-file-list) is suggested to be the professor's responsibility, and the rest are suggested to be the TA's.

\begin{enumerate}
\item {submit-file-list

This is a file that lists all the expected files in the student submission. For details, see \S\ref{sec:submit-list}.
}
\item {config\_hwX

This is the config file read by the automated grader. It is explained in more detail in \S\ref{sec:config}.
}
\item {run\_hwX.sh

When the grader runs student files, it calls this shell script. This script will be called from the top directory of the student's untarred submit file, so all paths should be relative to the student's folder.

The run file can serve three functions, one mandatory and two optional:

1. It \textit{MUST} rerun the commands that generate output the student turned in. These new output files should be put in the same location as the turned-in files with the same name, but with a \texttt{.out} appended. For example, if the student turns in a file at \texttt{Q2/lambda1.lm}, the run file should create a file \texttt{Q2/lambda1.lm.out}.

2. It may run student code on test input that the grader has created.

3. It may run any arbitrary scripts or functions that aid the grader.
}
\item {Gold files

For every student output, there must be a gold output file. For clarity's sake, I suggest giving them the extension \texttt{.gold}. If you have withheld input or test data, there also must be gold output for those cases.
}
\item {Withheld test input (optional)

If you are running student code on withheld test input (e.g., modular or targeted tests), these files need to be written.

}
\end{enumerate}

The submit-file-list (1) needs to be made available to students for homework checker, so should be put on \texttt{/dropbox} with the assignment.

The rest of the files should be saved and passed on to help future TAs. The suggestion is to put the rest into a TA/instructor-only grading folder for the course (inside \texttt{/dropbox/year/course/grading}, but this will be negotiated with the professor and grading team.

\subsection{Submit List} \label{sec:submit-list}

The submit-file-list lists all the files expected in the student's submitted tar file. The format is a set of newline-separated list of expected files, in relative path format. For instance, the file might look like the following:

\vspace{5pt}

\texttt{run.sh}

\texttt{table}

\texttt{Q2/q2.sh}

\texttt{Q2/decoder.sh}

\texttt{Q3/q3.sh}

\texttt{Q3/results/data.txt}

\subsection{Config File} \label{sec:config}

The config file is a plaintext file with configuration inputs separated by line. The format is similar to condor .cmd files, and there is no required order to the config file: each line stands on its own. The format and configuration lines are:

\vspace{5pt}

1. \texttt{zipfile = <zipfile name>}

\texttt{<zipfile name>} is the path to the zip file that was generated by Canvas when you downloaded the assignments.

\vspace{5pt}

2. \texttt{destination = <destination folder name>}

\texttt{<destination folder name>} is the local path to the folder where student submissions will be unzipped (for opening), or where the student submissions already exist (for evaluating).

\vspace{5pt}

3. \texttt{report = <name of report file>}

\texttt{<name of report file>} is the local path to the report file. If this line is omitted, the report file will be given the name \texttt{<destination folder>\_report.txt}.

\vspace{5pt}

4. \texttt{file\_structure = <submit-file-list>}

\texttt{<submit-file-list>} is the same file that the automated checker uses and that the professor creates for the assignment (see \S\ref{sec:submit-list}).

\vspace{5pt}

5. \texttt{ignore\_newlines = <true|false>}

\texttt{ignore\_newlines} tells the grading script whether to skip empty lines when comparing files. If omitted, it defaults to True.

\vspace{5pt}

6. \texttt{ignore\_whitespace = <true|false>}

\texttt{ignore\_whitespace} tells the grading script whether to tokenize individual lines by whitespace when comparing files. If set to False, the lines of each file are compared as-is with no processing, which blocks fuzzy matching (see below). If omitted, \texttt{ignore\_whitespace} defaults to True.

\vspace{5pt}

7. \texttt{fuzzy\_match = <float>}

\texttt{<float>} is a numerical value that specifies how different numeric values can be from one another when performing comparisons. For instance, a value of \texttt{0.1} means that student values can be up to 10\% larger or smaller than the value in the gold standard. Fuzzy matching only occurs if \texttt{ignore\_whitespace} is set to true. If omitted, \texttt{<float>} defaults to 0 (i.e., numeric values must be identical).

\vspace{5pt}

\noindent \textbf{The following config lines can be repeated multiple times.}

8. \texttt{repro\_comparison = <student\_file> <sorted|unsorted>}

This line should be repeated for as many student output files as the assignment requires. The grader will compare the file located at the \textit{relative} path \texttt{<student\_file>} to the expected file at \texttt{<student\_file>.out} (which should be generated by \texttt{run\_hwX.sh}).

You must specify whether the file should be \texttt{sorted} or left \texttt{unsorted} before comparison occurs. If the order of the output file is unimportant (for instance, it is the unordered output from a hash), then it is best to select \texttt{sorted}.

\vspace{5pt}
\newpage

9. \texttt{gold\_comparison = <gold\_file> <student\_file> <sorted|unsorted> <points>}

This line should be repeated for as many student output files as are being graded, including any test output generated by \texttt{run\_hwX.sh}. \texttt{<gold\_file>} is the \textit{absolute} path to the gold file, and \texttt{<student\_file>} is the \textit{relative} path to the student file.	

As with \texttt{repro\_comparison}, you must specify whether the files should first be \texttt{sorted} before comparing, or left \texttt{unsorted}.

You must also specify the integer point value \texttt{<points>} assigned to matching the gold file. This is a blunt comparison: the grader will either assign full points or no points. If you wish to assign partial points, it may be easiest to tell the grader it is worth 0 points and then manually alter the grade. The grader does not check that the \texttt{gold\_comparison} point values sum up to 75 (the points available for assignment after the standard portion is graded), so think carefully about how many points you want to assign to matching gold outputs.

\vspace{5pt}

\begin{comment}
\noindent \textbf{The following config lines are for future use (see \S\ref{sec:future}), and are not used by the current program.}

\vspace{5pt}

10. \texttt{run\_script = <run\_hwX.sh>}

This is the place to point the config file to the \texttt{run\_hwX.sh} file described in \S\ref{sec:fileslist}. Currently this is executed by a separate wrapper.

\vspace{5pt}

11. \texttt{cmd = <condor cmd file>}

\texttt{<condor cmd file>} is a generic condor file that is used to run \texttt{run\_script} for all students. It should probably never be changed from the \texttt{hw.cmd} file present in the grader directory. As with \#7, this is currently executed by a separate wrapper.

\vspace{5pt}
\end{comment}

\section{Automated Checker (check\_hw.py)} \label{sec:checker}

The automated checker is located under the src directory, and is for use by students. It confirms that their hw.tar.gz file has all expected files. It runs the same file-checking that the full grader does, but also checks for expected source code. The method for calling the code is:

\texttt{python check\_hw.py <languages> <submit-file-list> <tar\_name>}

\subsection{Checker dependencies}

\begin{enumerate}
\item {\texttt{<languages>}

This is a file that contains a list of recommended coding languages and their executable formats. The format is line-separated, and each line lists an executable file format, followed by the source code formats. That is, each line looks like:

\texttt{executable\_extension code\_extension1 code\_extension2 ...}

If a language is scripted, only the extension of the script appears, and there may be multiple lines with different executables. An example file could look like:

\vspace{5pt}

\texttt{exe cs}

\texttt{exe c h}

\texttt{class java}

\texttt{jar java}

\texttt{py}

\texttt{pl}

\vspace{5pt}

The master version of this file is called ``languages" and lives in the src directory with the check\_hw.py script.

}
\item {\texttt{<submit-file-list>}

This is the same file described in \ref{sec:submit-list}.

}
\item {\texttt{<tar\_name>}

This is the name of the tar file the homework checker looks for. Following current 57x policy, it should be hw.tar.gz.
}
\end{enumerate}

\subsection{Student-facing check\_hw.sh}

The check\_hw.py script is extensible and configurable. This is for flexibility for us, but not ideal for student use. The current policy is to create a wrapper, called check\_hw.sh, inside the directory of each homework assignment. E.g., a \texttt{check\_hw1.sh} in the hw1 directory that calls into \texttt{check\_hw.py} with the appropriate arguments, and so on.

\subsection{Behavior}

The check\_hw script performs the following checks:

\begin{itemize}
\item Checks to see if the tar extracts. If it doesn't, prints an error.
\item Checks for all files and folder structures listed in \texttt{submit-file-list}. Prints an error for each file and folder missing.
\item Checks for windows carriage returns in all files listed in \texttt{submit-file-list}. Prints an error if any are found.
\item Checks that all *.sh files begin with \#!/bin/sh. Prints an error for each *.sh file that doesn't.
\item Checks to see if there is some code from a language listed in \texttt{languages}. If not, prints a warning.
\item If the tar includes binaries, checks for (expected) corresponding source code. Prints a warning if not found. If found, it prints the binary file and its (assumed) source code.
\item If the script runs into an exception, it prints out some information and asks the user to email davinman@uw.edu with the tar file that triggered the exception.
\end{itemize}

\subsection{check\_hw Tests}

The tests I ran for check\_hw.py are under data/test\_hw\_checker. The directory contains a set of zip files with descriptive names, detailing what tests are run on each, and the expected results. To run all the tests, call test\_all.sh from within the data/test\_hw\_checker/ folder.

\section{Grading Steps} \label{sec:autograder}

After environment setup (\ref{sec:setup}), there are three steps to using the automatic grader: unpacking the zip file from Canvas and grading the presence of requested files in the submission (\ref{sec:unzipping}), running student code (\ref{sec:runningcode}), and grading the run code (\ref{sec:secondgrade}).

\subsection{Setup} \label{sec:setup}

The most important part of setting up is adding the version of the grader to your PATH. To do this permanently, you need to add the following line to your $\sim$/.profile file:

\vspace{5pt}

\texttt{export PATH=\$PATH:/dropbox/grading/grader57x/2.0/grader57x/scripts/}

\vspace{5pt}

To use another version of the grader, change the \texttt{2.0} in the path to the preferred version. Once you have added this to your $\sim$/.profile your PATH will be set every time you log in. To use it immediately (without logging out and back in), run

\vspace{5pt}

\texttt{source $\sim$/.profile}

\vspace{5pt}

For each homework, you will also need to set up the config file and gold files, as described in \S\ref{sec:files}.

\subsection{Unzipping the zip file and file checking} \label{sec:unzipping}

To open the zip file, use the parameter \texttt{open} when calling \texttt{grader.sh}:

\vspace{5pt}

\texttt{grader.sh <hw\_config> open}

\vspace{5pt}

This will expand the zip file and all students' tar files into the destination folder defined in \texttt{config\_file}.

The destination folder will have student submissions sorted into folders based on the student's name as given by Canvas. Each folder will be a concatenated string of the student's last, first, and middle names (in that order), and within each folder, the submitted tar file will be exploded.

Upon opening student folders, the grader will grade the ``Submitted Files" section of the standard portion of the rubric (\S\ref{sec:standardrubric}). This information will be written out to the report file defined in \texttt{config\_file}, which is sorted alphabetically by student name.

If a student submits their code late or separately, use the parameter \texttt{open\_student} followed by the student's name and their tar file. For example:

\vspace{5pt}

\texttt{grader.sh hw1\_config open\_student doejohn john\_hw.tar.gz}

\vspace{5pt}

This will add the contents of \texttt{john\_hw.tar.gz} to the folder containing other student submissions, under the subdirectory \texttt{doejohn}, and add their grade information into the report. This will integrate John Doe's submission with other student submissions, as if it had been in the original zip file.

This command cannot see if the student turned in the readme file, since readme files are turned in outside of the tar file. This script assumes the readme file is not present, if not in the tar file, and prints a warning.

\vspace{5pt}

\subsection{Running student code} \label{sec:runningcode}

Student code is run by calling:

\vspace{5pt}

\noindent \texttt{run\_all.sh <hw\_directory> <run\_script.sh>}

\vspace{5pt}

\noindent The \texttt{hw\_directory}  is the same as the \texttt{destination} directory in the config file (\S\ref{sec:config}), and \texttt{run\_script} is the script file you have created.

The \texttt{run\_all.sh} script loops through every student directory in \texttt{hw\_directory} and calls the \texttt{run\_script} through condor within the directory. It will launch X condor commands where X is the number of students.

To run a single student's code, the command is:

\vspace{5pt}

\noindent \texttt{run\_student.sh <student\_directory\_path> <run\_script.sh>}

\vspace{5pt}

Note that \texttt{student\_directory\_path} must be the \textit{full} path to the student directory, e.g. \texttt{hw4/doejohn/}.

\subsection{Evaluation} \label{sec:secondgrade}

After the condor scripts have all finished (you must manually check with \texttt{condor\_q <username>}), you can run the evaluation component of the automatic grader. This is done by calling:

\vspace{5pt}

\noindent \texttt{grader.sh <hw\_config> eval\_all}

\vspace{5pt}

This will launch all the comparisons defined in the config file (\S\ref{sec:config}). If any differences are found in the comparisons, a summary will be written to the report file, and diffs will be written to the student directories.

To run the evaluation on a single student, the command is:

\vspace{5pt}

\noindent \texttt{grader.sh <hw\_config> eval\_student <student\_directory\_name>}

\vspace{5pt}

\texttt{student\_directory\_name} is just the name of the student directory itself. That is, the parameter should be \texttt{doejohn}, not \texttt{hw4/doejohn}.

\textbf{Note:} The file comparisons can take a while, especially when calling \texttt{eval\_all}, so it may be useful to wrap this command inside of a condor script of its own.

\section{Code Documentation} \label{sec:documentation}

\subsection{Using Bitbucket} \label{sec:bitbucket}

The grader software is managed under Bitbucket at the repository \\ \texttt{https://bitbucket.org/davinman/grader57x}. If you wish to modify the code or check it out for yourself, you will need to create a Bitbucket account by going to \texttt{bitbucket.org} and request permission to edit the branch. You will also need a Bitbucket account to pull down a new version of the code and place it on Patas. I am currently using Mercurial as the management system.

Each stable version of the code is forked into its own branch. As of the publication of this document, there are three: 0.1, 1.0, and 2.0.

\subsection{Code Structure} \label{sec:structure}

The code for the automated grader is written in Python and located inside the grader57x folder.

The code is separated into three files. The first is the GradeReport.py class, which describes the object that hosts (potentially intermediate) student grades, which are then written out to file. The second is the Differ.py file, which is the class that does the diff comparison between student files. The last is grader.py, which is the code that interprets and executes command line calls (open, evaluate, etc), and initializes and populates the GradeReport and Differ objects.

The grader.py code is modularized as much as possible: This includes having the eval\_all execution iteratively call into eval\_student, so that the eval\_student command is guaranteed to produce the same output as eval\_all. This includes zeroing out parts of the grade report before evaluating (this ensures that the eval scripts can be run multiple times).

Reading the config file is done in the function read\_config\_file. The code is not complex, but it is important that it align with the expected config file parameters, and that it generate any additional implicit values in the config file (for instance, summing the points in the \texttt{gold\_comparison} sections).

The GradeReport object is initialized in two ways: the original initialization, which is done when unzipping the Canvas-generated file (under the unzip function), and reading the grade report on file (done under the open\_student, eval\_all, and eval\_single\_student functions). Generating the GradeReport from file is done by calling a class function within the GradeReport class.

\section{Potenial Future Developments} \label{sec:future}

This section lists the most important needs for future development.

\subsection{Internalizing the run script} \label{sec:internalrun}

Currently, the run script is separate from the \texttt{grader.sh} script. This is due to some difficulties Python has calling into condor. It would be more parsimonious if these difficulties could be overcome, and running the assignment could be called from \texttt{grader.sh} with a \texttt{run\_all} and \texttt{run\_student} parameter, just like the other two grading steps.

\subsection{Grading special lines in the output} \label{sec:speciallines}

For some assignments, only particular lines of the output need to be graded. It would be good to add a parameter that, for certain comparison operations, would only grade lines that had a leading string in them.

\end{document}
